<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
 <title>Tom Preston-Werner</title>
 <link href="http://tom.preston-werner.com/atom.xml" rel="self"/>
 <link href="http://tom.preston-werner.com/"/>
 <updated>2015-09-14T04:12:49-04:00</updated>
 <id>http://tom.preston-werner.com/</id>
 <author>
   <name>Tom Preston-Werner</name>
   <email>tom@mojombo.com</email>
 </author>

 
 <entry>
   <title>Savage's approach to research</title>
   <link href="http://tom.preston-werner.com/Savage's%20approach%20to%20research/"/>
   <updated>2015-09-10T00:00:00-04:00</updated>
   <id>http://tom.preston-werner.com/Savage's approach to research</id>
   <content type="html">&lt;p&gt;Savage’s approach to research, via Mosteller（copy from Jon McAuliffe’s and Jun Liu’s webpage):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As soon as a problem is stated, start right away to solve it. Use simple examples.&lt;/li&gt;
  &lt;li&gt;Keep starting from &lt;strong&gt;first principles&lt;/strong&gt;, explaining again and again what you are trying to do.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Believe&lt;/strong&gt; that this problem can be solved and that you will enjoy working it out.&lt;/li&gt;
  &lt;li&gt;Don’t be hampered by the original problem statement. Try other problems in its neighborhood; maybe there is a better problem than yours.&lt;/li&gt;
  &lt;li&gt;Work an hour or so on it frequently. Talk about it; explain it to people.&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Notes on Sure Screening interaction selection</title>
   <link href="http://tom.preston-werner.com/Notes%20on%20Sure%20screening%20interaction%20selection/"/>
   <updated>2015-09-08T00:00:00-04:00</updated>
   <id>http://tom.preston-werner.com/Notes on Sure screening interaction selection</id>
   <content type="html">&lt;h4 id=&quot;lee-s-lozano-a-kambadur-p--xing-e-p-2015-april-an-efficient-nonlinear-regression-approach-for-genome-wide-detection-of-marginal-and-interacting-genetic-variations&quot;&gt;Lee, S., Lozano, A., Kambadur, P., &amp;amp; Xing, E. P. (2015, April). An Efficient Nonlinear Regression Approach for Genome-Wide Detection of Marginal and Interacting Genetic Variations.&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Summary in my own understanding:&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;A screening algorithm based on iterative sure independent screening theory.
    &lt;ul&gt;
      &lt;li&gt;The marginal and the pairs are all evaluated using cheap method. And the top features or pairs are selected. &lt;strong&gt;&lt;em&gt;Although this save a lot of computation, for high order interactions more than 2nd order, it is still exponential that is not acceptable.&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;After the few SNPs and interactions are selected. We solve the effects of each SNPs or interactions. To consider the nonlinear relationship of interactions. The piecewise linear model is utilized.&lt;/li&gt;
  &lt;li&gt;To control the false positive, for different parameter, we randomly sampled &lt;script type=&quot;math/tex&quot;&gt;\frac{N}{2}​&lt;/script&gt; samples and run piecewise linear model on this part of data.
    &lt;ul&gt;
      &lt;li&gt;As the number of false positive can be bounded according to our parameter &lt;strong&gt;&lt;em&gt;supported by stable selection theory&lt;/em&gt;&lt;/strong&gt;, we choose a small subset of SNPs and pairs that satisfy  the false positive requirement.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Evaluation Metrics
    &lt;ul&gt;
      &lt;li&gt;False positive control&lt;/li&gt;
      &lt;li&gt;Statistical power&lt;/li&gt;
      &lt;li&gt;Screening scalability&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;fan-y-kong-y-li-d--zheng-z-2015-innovated-interaction-screening-for-high-dimensional-nonlinear-classificationthe-annals-of-statistics433-1243-1272&quot;&gt;Fan, Y., Kong, Y., Li, D., &amp;amp; Zheng, Z. (2015). Innovated interaction screening for high-dimensional nonlinear classification. &lt;em&gt;The Annals of Statistics&lt;/em&gt;, &lt;em&gt;43&lt;/em&gt;(3), 1243-1272.&lt;/h4&gt;

&lt;p&gt;Summary in my own understanding&lt;/p&gt;

&lt;h6 id=&quot;some-assumptions&quot;&gt;Some assumptions:&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;The theory works for Gaussian &lt;strong&gt;two-class Gaussian classification&lt;/strong&gt; problem.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;z=\triangle z_1 + (1-\triangle)z_2\\
z_1 \text{ and } z_2 \sim N(\mu_1,\Sigma_1) \text{ and } N(\mu_2, \Sigma_2) \text{ respectively}\\
\triangle \sim Ber(\pi)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The min and max eigenvalue of covariance if bounded.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tau_1\leq \lambda_{min}(\Sigma_k)\leq\lambda_{max}(\Sigma_k)\leq\tau_p&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;The variance of different class can be distinguished.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\Sigma_1^{-1}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\Sigma_2^{-1}&lt;/script&gt; are both &lt;script type=&quot;math/tex&quot;&gt;K_p&lt;/script&gt; sparse. And &lt;script type=&quot;math/tex&quot;&gt;l_\infty(\Sigma_k^{-1})&lt;/script&gt; is bounded.&lt;/li&gt;
  &lt;li&gt;​&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;methods&quot;&gt;Methods:&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;Reduce the number of interactions to a moderate order by a new inter- action screening approach&lt;/li&gt;
  &lt;li&gt;Identify both important main effects and interactions using some variable selection techniques&lt;/li&gt;
  &lt;li&gt;An interaction screening criteria that can be proved to enjoy sure screening property.&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;interaction-screening-method&quot;&gt;Interaction screening method&lt;/h6&gt;

&lt;ol&gt;
  &lt;li&gt;For binary classification problem, the importance of single feature to interaction is attributed as the difference of variance of two classes.
    &lt;ul&gt;
      &lt;li&gt;Transform the feature space by using &lt;script type=&quot;math/tex&quot;&gt;\Sigma_1^{-1}x&lt;/script&gt;  (where &lt;script type=&quot;math/tex&quot;&gt;\Sigma_1&lt;/script&gt; denote the covariance of samples in class &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;). And similarly, we also transform the original features using &lt;script type=&quot;math/tex&quot;&gt;\Sigma_2^{-1}x&lt;/script&gt; .&lt;/li&gt;
      &lt;li&gt;In the new feature space, the difference between variance of two class represents how important the single feature is for interaction.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h6 id=&quot;sure-screening-property&quot;&gt;Sure screening property&lt;/h6&gt;

&lt;h6 id=&quot;when-sigma-1-is-known&quot;&gt;When &lt;script type=&quot;math/tex&quot;&gt;\Sigma^{-1}&lt;/script&gt; is known&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;​&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Readlist of Distributed Optimization Algorithm</title>
   <link href="http://tom.preston-werner.com/ReadList%20of%20Distributed%20Optimization%20Algorithms/"/>
   <updated>2015-05-10T00:00:00-04:00</updated>
   <id>http://tom.preston-werner.com/ReadList of Distributed Optimization Algorithms</id>
   <content type="html">&lt;p&gt;[1] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trend in Machine Learning, 3(1):1–122, 2011.&lt;/p&gt;

&lt;p&gt;[2] Joseph K Bradley, Aapo Kyrola, Danny Bickson, and Carlos Guestrin. Par- allel coordinate descent for l1-regularized loss minimization. arXiv preprint arXiv:1105.5379, 2011.&lt;/p&gt;

&lt;p&gt;[3] Weizhu Chen, Zhenghao Wang, and Jingren Zhou. Large-scale l-bfgs using mapreduce. In Advances in Neural Information Processing Systems, pages 1332–1340, 2014.&lt;/p&gt;

&lt;p&gt;[4] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in Neural Information Processing Systems, pages 1223–1231, 2012.&lt;/p&gt;

&lt;p&gt;[5] Yi Wang, Hongjie Bai, Matt Stanton, Wen-Yen Chen, and Edward Y Chang. Plda: Parallel latent dirichlet allocation for large-scale applications. In Al- gorithmic aspects in information and management, pages 301–314. Springer, 2009.&lt;/p&gt;

&lt;p&gt;[6] Yong Zhuang, Wei-Sheng Chin, Yu-Chin Juan, and Chih-Jen Lin. Dis- tributed newton method for regularized logistic regression. Department of Computer Science and Information Engineering, National Taiwan University, Tech. Rep, 2014.&lt;/p&gt;

&lt;p&gt;[7] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Par- allelized stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 2595–2603, 2010.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Differential Privacy and Machine Learning</title>
   <link href="http://tom.preston-werner.com/differential%20privacy%20and%20machine%20learning/"/>
   <updated>2015-03-11T00:00:00-04:00</updated>
   <id>http://tom.preston-werner.com/differential privacy and machine learning</id>
   <content type="html">&lt;p&gt;As a begining of my research, I survey the topics on differential privacy and machine learning. The basic structure of this slide is arised from [1].&lt;/p&gt;

&lt;p&gt;Some very important topics in differential privacy machine learning containing in&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The local sensitivity and smooth sensitivity.&lt;/li&gt;
  &lt;li&gt;The utility of mechansim.&lt;/li&gt;
  &lt;li&gt;The relationship of differential privacy with learning theory.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;is not covered in this slides.&lt;/p&gt;

&lt;p&gt;The slides: click here &lt;a href=&quot;https://github.com/boliu68/boliu68.github.io/blob/master/res/differential_privacy_machine_learing.pdf?raw=true&quot;&gt;2&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Referenecs:&lt;/p&gt;

&lt;p&gt;[1] Ji, Z., Lipton, Z. C., &amp;amp; Elkan, C. (2014). Differential Privacy and Machine Learning: a Survey and Review. Learning; Cryptography and Security; Databases.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>LR, Spark, SGD and Big Data</title>
   <link href="http://tom.preston-werner.com/LR,%20Spark,%20SGD%20and%20Big%20Data/"/>
   <updated>2014-12-04T00:00:00-05:00</updated>
   <id>http://tom.preston-werner.com/LR, Spark, SGD and Big Data</id>
   <content type="html">&lt;p&gt;Logistic Regression(&lt;a href=&quot;http://en.wikipedia.org/wiki/Logistic_regression&quot;&gt;LR&lt;/a&gt;) serves as a simple but competitive algorithm for classifiation. LR is effective to train and widely utilized online for recommendation, ads and rankings. In this blog, I will demonstrate my experience of LR algorithm based on Stochastic Gradient Descent(&lt;a href=&quot;http://en.wikipedia.org/wiki/Stochastic_gradient_descent&quot;&gt;SGD&lt;/a&gt;), &lt;a href=&quot;https://spark.apache.org/&quot;&gt;Spark&lt;/a&gt; for quite big dataset. The method is based on python. Some python packages such as numpy, scipy as well as sklearn is utlized for efficiency. It will not be difficult to transfer to other platform, in my opinion.&lt;/p&gt;

&lt;p&gt;The so called big data, in this post, represents billions instance(10e9) and billions dimensions(10e8). No doubt that the dimension is very very sparse. On average, only 64 non-zero entries exist for each instance. Concerned about many issuse ^-^. I only talks about pseudo-cdoe as follows.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Spark_LR(dataset)
#dataset: #instnaces * #dimension
#parameters: #partitions,#parallelism
1 Read data and split each line, repartition the dataset
2 Build the hash map for features
3 Dummy(intersection) the features and record the non-zero coordinate/value
4
5 #building csr_matrix in scipy is expensive in both memory and computation.
6 data_rdd = spark.MapPartitions(convert to scipy.csr_matrix).cache()
7 count the #instance, #dimension etc.
8
9 while not converge
10  #depend on l1 or l2 norm is used
11 	para_rdd = data_rdd.MapPartitions(SGD(using sklearn))
12	average all the parameters learnt by SGD
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The pseudo-code is very very simple. But it cost me a lot of time to make this work on billions instances. Based on some following experience, LR can handle billions-dataset on a cluster with 240 cpu cores and 640 GB memory in total. However, the disk space is also a bottleneck that I will metion later.&lt;/p&gt;

&lt;h2 id=&quot;tips-and-experiences&quot;&gt;Tips and experiences:&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Line 1&lt;/strong&gt;: repartition() function in pyspark results in a more balance partitions. However, repartition requires too much computation, memory as well as disk for shuffle. After several attempts, I will never utilize repartition() directly on such large dataset.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Line 3&lt;/strong&gt;: Maybe particular for python using scipy.sparse package. Building csr_matrix is quite expensive. Thus, instead of building csr_matrix for each instance again and again. We will directly map each partition to a big csr_matrix.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Line 6&lt;/strong&gt;: Converting the large dataset to a big csr_matrix run out of memory several times as well. Thus, in the final version code, I serialize to building several smaller csr_matrix and concat in the last. This will trade some time for memory. But I believe it is worth as you cannot guarantee how large the dataset will be.&lt;/p&gt;

&lt;p&gt;Storage.MEMORY_AND_DISK is suggested in my opinion to persist. Intuitively, this results in crazy usage of disk space. However, compared to the usage by shuffle metioned later, this is acceptable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Line 11&lt;/strong&gt;: I just call sklearn.linear_model.SGDClassifier to do SGD in each partitions. No doubt that you can implement one yourself which should be more fun.&lt;/p&gt;

&lt;p&gt;One of biggest challenge here is that the high-dimension(10e8) parameters vectors obtained from each partitions has to be averaged. Calling sum(), reduce() lead to calling a collect(). This means, in my experiment, collecting 100GB sparse vector back to master. Obviously, this is not acceptable. As a result, for &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; obtain in partition &lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;, I partition &lt;script type=&quot;math/tex&quot;&gt;w_i&lt;/script&gt; to #parallesim. For instance &lt;script type=&quot;math/tex&quot;&gt;w_i \rightarrow (1,w\_{i,1}), (2,w\_{i,2})\dots(k,w\_{i,k})&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; denotes the k which is identical for all the partitions. Afterwards, reduceByKey is called to sum up all the resul and only one parameter vector is collected back.&lt;/p&gt;

&lt;p&gt;Furthermore, a serious side effect is that we change the key in doing the above trick. And each time, for instance, 100GB(400 partitions * 300MB sparse vector) hash to be shuffled and write to disk. For really many times, my disk is running out. I try to compress the shuffle by flush the shuffle files to hdfs. But both does not work well.&lt;/p&gt;

&lt;p&gt;No doubt that fewer partition will alleviate this problems. However, few partitions means that you have to use fewer cpus to avoid the memory is running out. Quite several time have to be attempted.&lt;/p&gt;

&lt;p&gt;In total, to avoid keeping all the dataset in memory, I follows the guidance in &lt;a href=&quot;http://blog.smola.org/post/977927287/parallel-stochastic-gradient-descent&quot;&gt;blog of Alex Smola&lt;/a&gt; that&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We overpartition the dataset and using a fixed learning rate for SGD.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is the mechanism I implements, the majority of time is costed on tricks and debugging. I have no idea how to test and dubug when the problems can only occur when large dataset.&lt;/p&gt;

&lt;p&gt;Moreover, I would be appreciated if you can offer me some discussion or hints caused, currently, the disk usage, memory usage and cpu usage is not perfect. Only very few cpus are used for memory concerne. Moreover, if you can inspire me to avoid the shuffling of realling huge data of parameters vectors. I will also be appreciated.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Readlist of Cross-Domain Collaborative Filtering</title>
   <link href="http://tom.preston-werner.com/CDCF%20Reading%20list/"/>
   <updated>2014-11-19T00:00:00-05:00</updated>
   <id>http://tom.preston-werner.com/CDCF Reading list</id>
   <content type="html">&lt;p&gt;❏ Transfer RecSys
This lists the paper recently I plan to read. The majority of the following papers covers Cross-Domain Collaborative Filtering(CDCF).&lt;/p&gt;

&lt;p&gt;✔ Cremonesi, P., &amp;amp; Quadrana, M. (2014, October). Cross-domain recommendations without overlapping data: myth or reality?. In Proceedings of the 8th ACM Conference on Recommender systems (pp. 297-300). ACM.&lt;/p&gt;

&lt;p&gt;❏ Li, C. Y., &amp;amp; Lin, S. D. (2014, August). Matching users and items across domains to improve the recommendation quality. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 801-810). ACM.&lt;/p&gt;

&lt;p&gt;❏ Li, B., Yang, Q., &amp;amp; Xue, X. (2009, July). Can Movies and Books Collaborate? Cross-Domain Collaborative Filtering for Sparsity Reduction. In IJCAI (Vol. 9, pp. 2052-2057).&lt;/p&gt;

&lt;p&gt;✔ Li, B. (2011, November). Cross-domain collaborative filtering: A brief survey. In Tools with Artificial Intelligence (ICTAI), 2011 23rd IEEE International Conference on (pp. 1085-1086). IEEE.&lt;/p&gt;

&lt;p&gt;✔Gao, S., Luo, H., Chen, D., Li, S., Gallinari, P., &amp;amp; Guo, J. (2013). Cross-Domain Recommendation via Cluster-Level Latent Factor Model. In Machine Learning and Knowledge Discovery in Databases (pp. 161-176). Springer Berlin Heidelberg.&lt;/p&gt;

&lt;p&gt;❏ Li, B., Yang, Q., &amp;amp; Xue, X. (2009, June). Transfer learning for collaborative filtering via a rating-matrix generative model. In Proceedings of the 26th Annual International Conference on Machine Learning (pp. 617-624). ACM.&lt;/p&gt;

&lt;p&gt;❏ Moreno, O., Shapira, B., Rokach, L., &amp;amp; Shani, G. (2012, October). Talmud: transfer learning for multiple domains. In Proceedings of the 21st ACM international conference on Information and knowledge management (pp. 425-434). ACM.&lt;/p&gt;

&lt;p&gt;❏ Li, B., Yang, Q., &amp;amp; Xue, X. (2009, July). Can Movies and Books Collaborate? Cross-Domain Collaborative Filtering for Sparsity Reduction. In IJCAI (Vol. 9, pp. 2052-2057).&lt;/p&gt;

&lt;p&gt;✔ Loni, B., Shi, Y., Larson, M., &amp;amp; Hanjalic, A. (2014). Cross-Domain Collaborative Filtering with Factorization Machines. In Advances in Information Retrieval (pp. 656-661). Springer International Publishing.&lt;/p&gt;

&lt;p&gt;❏ Fernández-Tobías, I., Cantador, I., Kaminskas, M., &amp;amp; Ricci, F. (2012). Cross-domain recommender systems: A survey of the state of the art. In Proceedings of the 2nd Spanish Conference on Information Retrieval. CERI.&lt;/p&gt;

&lt;p&gt;❏ Pan, W., &amp;amp; Ming, Z. (2014). Interaction-Rich Transfer Learning for Collaborative Filtering with Heterogeneous User Feedbacks. IEEE Intelligent Systems, 1.&lt;/p&gt;

&lt;p&gt;❏Shi, Y., Larson, M., &amp;amp; Hanjalic, A. (2014). Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges. ACM Computing Surveys (CSUR), 47(1), 3.&lt;/p&gt;

&lt;p&gt;❏ Low, Y., Agarwal, D., &amp;amp; Smola, A. J. (2011, August). Multiple domain user personalization. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 123-131). ACM.&lt;/p&gt;

&lt;p&gt;❏ Berkovsky, S., Goldwasser, D., Kuflik, T., &amp;amp; Ricci, F. (2006, May). Identifying Inter-Domain Similarities Through Content-Based Analysis of Hierarchical Web-Directories. In ECAI (pp. 789-790).&lt;/p&gt;

&lt;p&gt;❏ Dr. Weike Pan’ paper s&lt;/p&gt;

&lt;p&gt;❏ Hu, L., Cao, J., Xu, G., Wang, J., Gu, Z., &amp;amp; Cao, L. (2013, August). Cross-domain collaborative filtering via bilinear multilevel analysis. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence (pp. 2626-2632). AAAI Press.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>How to compile opencv on linux without root</title>
   <link href="http://tom.preston-werner.com/how%20to%20compile%20OpenCV%20without%20root/"/>
   <updated>2014-10-28T00:00:00-04:00</updated>
   <id>http://tom.preston-werner.com/how to compile OpenCV without root</id>
   <content type="html">&lt;h2 id=&quot;update-cmake&quot;&gt;update Cmake&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;only cmake version &amp;gt; 2.88 can be used to compile opencv.&lt;/li&gt;
  &lt;li&gt;Download latest version cmake source code and cd in the folder.&lt;/li&gt;
  &lt;li&gt;./bootstrap&lt;/li&gt;
  &lt;li&gt;make (No need to make install, just executate new cmake from built dir)&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;p&gt;##Compiling OpenCV&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Download the latest version opencv from website.&lt;/li&gt;
  &lt;li&gt;Mkdir build inside the unzip opencv folder.&lt;/li&gt;
  &lt;li&gt;cmake(execute the newly installed cmake using direct path)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The command(do not ignore &lt;strong&gt;..&lt;/strong&gt;):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=my_location -D BUILD_NEW_PYTHON_SUPPORT=ON ..
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;make&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;make install&lt;/li&gt;
  &lt;li&gt;Guarantee that python2.7 folder is generated in lib.&lt;/li&gt;
  &lt;li&gt;config the environment.&lt;/li&gt;
&lt;/ol&gt;
</content>
 </entry>
 
 <entry>
   <title>What mean by sparsity?</title>
   <link href="http://tom.preston-werner.com/What%20means%20by%20Sparsity%3F/"/>
   <updated>2014-10-14T00:00:00-04:00</updated>
   <id>http://tom.preston-werner.com/What means by Sparsity?</id>
   <content type="html">&lt;p&gt;In machine learning area, we hear a lot about sparsity such as compressed sensing, sparse coding, data sparsity and so on. Amongst, I am particular interested in data sparsity. In recommender system, the large proportion of missing ratings is called sparsity. On the other hand, the bag of word(BoG) represented document dataset is also named sparse data. What exactly means by sparsity is the fundamental problem for us to understand as well as utilize data sparsity. My original confusion is posted on &lt;a href=&quot;http://stats.stackexchange.com/questions/113318/the-name-data-sparsity-in-different-applications&quot; title=&quot;Cross Validated&quot;&gt;Cross Validated&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this blog, I hope to list the taxonomy and several definition from very intuitive perspective. Moreover, I would like to emphasize that this blog only talks about sparse data instead of sparse learning. The object method aims at learning a sparse parameter.&lt;/p&gt;

&lt;p&gt;First of all, I want to clarify two types of data sparsity&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Paucity of dataset particular training data.&lt;/li&gt;
  &lt;li&gt;High dimension feature space and only few dimensions are informative for each instance.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Paucity of training data&lt;/strong&gt;: Take the &lt;a href=&quot;http://en.wikipedia.org/wiki/Netflix_Prize&quot;&gt;Netflix Prize&lt;/a&gt; dataset as example again. The large proportion of missing ratings makes recovering the user-movie matrix more challenging. The sparsity ratio, &lt;script type=&quot;math/tex&quot;&gt;\frac{\#observerd}{\# user \times \# movies}&lt;/script&gt;, is always utilized how sparse the dataset is. Lee, J&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; provides an empirical analysis the relation between performance and density/sparisty. The insufficient training data situaiton is quite common.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;High dimension feature space&lt;/strong&gt;: Take the linear classifier for binary classification as example. As we known, the VC dimension of linear classifier is &lt;script type=&quot;math/tex&quot;&gt;O(d)&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; is feature dimension. As the dimension increase, the number of instance is required to grow as well.&lt;/p&gt;

&lt;p&gt;The high dimensionality also results in some other problems known as &lt;strong&gt;curse of dimensionality&lt;/strong&gt;. The sparsity is only one of consequences of curse of dimension. The distance becomes meaningless as dimension increase. However, the discusion of curse of dimension is beyond this post.&lt;/p&gt;

&lt;p&gt;In text mining, bag of word is always utilized to represent each domument like the following.&lt;/p&gt;

&lt;p&gt;\[[0,1,0,0,0\dots,0,0,0,\dots,1,0,\dots]\]&lt;/p&gt;

&lt;p&gt;We usually call such instance sparse caused only few entries are non-zero. However, such understanding is quite on the surface. As I actually cannot explain the difference between &lt;script type=&quot;math/tex&quot;&gt;[0,1,0,0,1]&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;[1,2,1,1,2]$.&lt;/script&gt; However, the latter one will not be called sparse. Duchi, J.&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; provides a formal condition for sparse as 
\[\text{for convex loss function } f,supp(f) \subset supp(\nabla f)\]&lt;/p&gt;

&lt;p&gt;From intuitive perspective, such condition means that the &lt;script type=&quot;math/tex&quot;&gt;i\_{th}&lt;/script&gt; of instance dimension &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; is zero &lt;script type=&quot;math/tex&quot;&gt;x_i=0&lt;/script&gt;, then the gradient with respect to &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; is deemed to be zero. Consider using stochastic gradient descent to learn the model. The instance with sparse features $x_i=0$ will not update &lt;script type=&quot;math/tex&quot;&gt;i\_{th}&lt;/script&gt; dimension at all. Duchi,&lt;/p&gt;

&lt;p&gt;J.&lt;sup id=&quot;fnref:3:1&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; also provides an analysis of stochastic optimization on such sparse data. When learning on such sparse dataset, we would like to learn an dense model rather than sparse model(assumption). Such assmption leads much harder to learn model on sparse data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Other definition&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;At the very begining of my survey. I always try to make an uniform definition for the above two type of sparsity. Actually, there indeed exist some common parts.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Both the paucity of training data and high dimension will cause the average distance between each data point large.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;http://math.stackexchange.com/questions/283006/what-is-a-sampling-density-why-is-the-sampling-density-proportional-to-n-fra&quot;&gt;sample density&lt;/a&gt; is also tried to measure the sparsity.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the following, I try to quote several “definition” in other references.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;Data sparsity refers to the difficulty in finding sufficient reliable similar users since in general the active users only rated a small portion of items&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;data sparsity problem occus in the setting of supervised statistical learning method, when some data from the test side is not present in the training dataset. &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;when the training data is relatively sparse in this domain, either because few observations are available for learning or because the underlying structure is complex, the bias inherent in popular pruning methods is inappropriate and they have a negative effect on predictive accuracy&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;p&gt;One way to think of sparsity is how space is empty (60%), whereas 40% of space is dense, or filled. So good data looks like swiss cheese.  Big bubbles of nothing&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;!&lt;/p&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There exist quite a lot definition or understanding of data sparsity. As far as I have survied, I do not find a very persuasive and comprehensive work to clarify all of these definitions.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;David Donoho, Sparsity in Modern High-Dimensional Statistics, SAMSI Astrostatistics. 20,9,2012 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Lee, J., Sun, M., &amp;amp; Lebanon, G. (2012). A comparative study of collaborative filtering algorithms. arXiv preprint arXiv:1205.3193. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;Duchi, J., Jordan, M., &amp;amp; McMahan, B. (2013). Estimation, optimization, and parallelism when data is sparse. In Advances in Neural Information Processing Systems (pp. 2832-2840). &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:3:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;Guo, G. (2013, August). Improving the performance of recommender systems by alleviating the data sparsity and cold start problems. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence (pp. 3217-3218). AAAI Press. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;Ricci, F., Rokach, L., &amp;amp; Shapira, B. (2011). Introduction to recommender systems handbook (pp. 1-35). Springer US. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;when the training data is relatively sparse in this domain, either because few observations are available for learning or because the underlying structure is complex, the bias inherent in popular pruning methods is inappropriate and they have a negative effect on predictive accuracy. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;http://www.quora.com/What-is-a-clear-explanation-of-data-sparsity &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</content>
 </entry>
 
 
</feed>