<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Bo Liu's Blog - Reinforcement learning versus bandit</title>
  <link rel="shortcut icon" href="/assets/images/favicon.ico">
  <link rel="stylesheet" href="/assets/css/style.css">
  <link rel="alternate" type="application/rss+xml" title="My Blog" href="/rss.xml">
  <link rel="stylesheet" href="/assets/css/highlight.css">
</head>
<body>

  <nav class="main-nav">
    
        <a href="/"> <span class="arrow">←</span> Home </a>
    

    
        
            <a href="/about">About </a>
        
    
    <a class="cta" href="/feed.xml">Subscribe</a>
</nav>

  

  <section id="wrapper" class="">
    <article class="post">
    <header>
        <h1>Reinforcement learning versus bandit</h1>
        <h2 class="headline">March 13, 2017</h2>
    </header>
    <section id="post-body">
        <h2 id="reinforcement-learning-versus-bandit">Reinforcement Learning versus Bandit</h2>

<p>Recently, I fulfill my PhD Qulifying Exam which focuses on Bandit and its application on online recommender system (RecSys). My supervisor keeps challenging the motivation of modeling RecSys as bandit problem compared to other Reinforcement Learning formulations. Caused quite few detail explanations are found online, I would like to share my understanding of what differs bandit from other RL formulation in this post.</p>

<p>The exploitation-exploration tradeoff is emphasized when tackling canonical problem of sequential decision making under uncertainty. More concretely, an agent sequentially interacts with unkown environment and receive the rewards as shown in Fig. 1. The ultimate objective is to maximize the cumulative reward. For one thing, the agent exploits known optimal action. For another, the agent explores unkown environment. Exploitation-exploration tradeoff is always formalized as Reinforcement Learning including Multi-Armed Bandit (MAB), Markov Decision Process (MDP), or Partially observable Markov Decision Process (POMDP). Our question is for what applications bandit forumation is more suitable.</p>

<p><img src="//github.com/boliu68/boliu68.github.io/blob/master/_posts/decision_making_uncertainty.png?raw=true" alt="Fig.1 Sequential decision making under uncertainty" /></p>

<p>According to many tutorials of Reinforcement Learning, the unknown environment can be described by multiple elements including observations <script type="math/tex">\Omega</script>, states <script type="math/tex">S</script>, actions <script type="math/tex">A</script>, reward function <script type="math/tex">R</script>, state transition probability <script type="math/tex">T</script>, and conditional observation probabilities <script type="math/tex">O</script>. Bandit, MDP, and POMDP models the environment by considering different elements respectively as shown in Fig. 2. More concretely, Bandit only explores which actions are more optimal regardless of state. Actually, the classical multi-armed bandit policies assume the i.i.d. reward for each action (arm) in all time. [1] also names bandit as one-state or stateless reinforcement learning and discuss the relationship among bandit, MDP, RL, and decision theory as shown in Fig. 3.</p>

<p><img src="//github.com/boliu68/boliu68.github.io/blob/master/_posts/bandit_mdp_pomdp.png?raw=true" alt="Fig. 2. Bandit vs MDP vs POMDP" />
<img src="//github.com/boliu68/boliu68.github.io/blob/master/_posts/one_state_rl.png?raw=true" alt="Fig 3. One-state Reinforcement Learning" /></p>

<p>Compared to one-state RL, i.e. multi-armed bandit. MDP explores not only the stochastic reward function but also the state and state transition probability. POMDP further generalizes MDP by assuming the hidden state and exploring the observations and observation state probability. We use the restaurant selection problem as example. We may often go to our favorite restaurant and try some new sometimes to find a potential better one. If our preference for every restaurant never changes, the bandit is very suitable formulation. If our preference changes to Chinese food after having Japanese food, then modeling and exploring the state transition is more suitable. Finally, if the preference of restaurant are decided by many hidden factors, e.g. hidden interest, mood, and etc, it is more preferable to consider POMDP.</p>

<p>Multi-armed bandit is widely applied to online recommendation to make personlized article recommenation [Li, Lihong]. Intuitively, user interests change very quickly. For instance, one user are highly unlikely to buy a Playstation 4 if have already bought one. As a result, it seems that modeling recommendation as MDP or POMDP is more suitable than bandit problem.</p>

<p>However, in my opinion, MDP and bandit focus on exploring different elements and do not conflict. We take Q-Learning as example as shown in Fig. 4. I suppose that line 7 explores the state transition. If we always select the action with maximum expected reward in each time step, Q-learning is equivalent to pure exploitation. Pure exploitation can stuck in suboptimal actions for stochastic rewards. For instance, in state <script type="math/tex">s_1</script> with two actions, a randomly generated large reward may mislead the policy and stuck in suboptimal action. As a result, in Line 5, Q-Learning select the actions using <script type="math/tex">\epsilon-greedy</script> which is most basic bandit policy.<br />
<img src="//webdocs.cs.ualberta.ca/~sutton/book/ebook/pseudotmp9.png" alt="Fig. 4 Q-learning" /></p>

<p>For online recommender system, in my opinion, (PO)MDP focuses on exploring user interests change and bandit focuses on exploring stochastic reward. (PO)MDP and bandit combined should lead to a better result.</p>

<ol>
  <li>Zhou, Li. “A survey on contextual multi-armed bandits.” CoRR, abs/1508.03326 (2015).</li>
  <li>Li, Lihong, et al. “A contextual-bandit approach to personalized news article recommendation.” Proceedings of the 19th international conference on World wide web. ACM, 2010.</li>
</ol>

    </section>
</article>
<footer id="post-meta" class="clearfix">
    <a href="http://twitter.com/YourTwitterUsername">
        <img class="avatar" src="/assets/images/boliu.jpg">
        <div>
            <span class="dark">Bo Liu</span>
            <span>Learning, Experiencing and Writing</span>
        </div>
    </a>

    <section id="sharing">
        <a class="twitter" href="https://twitter.com/intent/tweet?text=https://boliu68.github.io//2017/Reinforcement-Learning-versus-Bandit/ - Reinforcement learning versus bandit by @YourTwitterUsername"><span class="icon-twitter"> Tweet</span></a>

<a class="facebook" href="#" onclick="
    window.open(
      'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
      'facebook-share-dialog',
      'width=626,height=436');
    return false;"><span class="icon-facebook-rect"> Share</span>
</a>
    </section>
</footer>

<!-- Disqus comments -->

    <div class="archive readmore">
        <h3>Comments</h3>
        <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'boliu68githubio';
        var disqus_developer = 0; // developer mode is on
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>
    </div>


<!-- Archive post list -->


  </section>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script src="/assets/js/main.js"></script>
  <script src="/assets/js/highlight.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-67871118-1', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>



