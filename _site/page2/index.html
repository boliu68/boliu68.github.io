<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Latest Posts &#8211; Long Long Later</title>
<meta name="description" content="Describe this nonsense.">
<meta name="keywords" content="Jekyll, theme, themes, responsive, blog, modern">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Latest Posts">
<meta property="og:description" content="Describe this nonsense.">
<meta property="og:url" content="http://boliu68.github.io/page2/index.html">
<meta property="og:site_name" content="Long Long Later">





<link rel="canonical" href="http://boliu68.github.io/page2/">
<link href="http://boliu68.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Long Long Later Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://boliu68.github.io/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://boliu68.github.io/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://boliu68.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://boliu68.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://boliu68.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://boliu68.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://boliu68.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://boliu68.github.io/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post-index" class="feature">

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://boliu68.github.io/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://boliu68.github.io/images/head.jpg" alt="Bo Liu photo" class="author-photo">
					<h4>Bo Liu</h4>
					<p>Machine Learning, PhD student</p>
				</li>
				<li><a href="http://boliu68.github.io/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:6liubo8@gmail.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				
				
				<li>
					<a href="http://linkedin.com/in/https://hk.linkedin.com/pub/bo-brody-liu/55/52b/10b"><i class="fa fa-fw fa-linkedin"></i> LinkedIn</a>
				</li>
				
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://boliu68.github.io/posts/">All Posts</a></li>
				<li><a href="http://boliu68.github.io/tags/">All Tags</a></li>
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->


<div class="entry-header">
  <div class="image-credit">Image source: <a href="http://www.dargadgetz.com/ios-7-abstract-wallpaper-pack-for-iphone-5-and-ipod-touch-retina/">dargadgetz</a></div><!-- /.image-credit -->
  
    <div class="entry-image">
      <img src="http://boliu68.github.io/images/abstract-1.jpg" alt="Latest Posts">
    </div><!-- /.entry-image -->
  
  <div class="header-title">
    <div class="header-title-wrap">
      <h1>Long Long Later</h1>
      <h2>Latest Posts</h2>
    </div><!-- /.header-title-wrap -->
  </div><!-- /.header-title -->
</div><!-- /.entry-header -->

<div id="main" role="main">
  
<article class="hentry">
  <header>
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2014-11-19T00:00:00-05:00"><a href="http://boliu68.github.io/CDCF%20Reading%20list/">November 19, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://boliu68.github.io/about/" title="About Bo Liu">Bo Liu</a></span></span>
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
        Reading time ~2 minutes
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://boliu68.github.io/CDCF%20Reading%20list/" rel="bookmark" title="Readlist of Cross-Domain Collaborative Filtering" itemprop="url">Readlist of Cross-Domain Collaborative Filtering</a></h1>
    
  </header>
  <div class="entry-content">
    <p>❏ Transfer RecSys
This lists the paper recently I plan to read. The majority of the following papers covers Cross-Domain Collaborative Filtering(CDCF).</p>

<p>✔ Cremonesi, P., &amp; Quadrana, M. (2014, October). Cross-domain recommendations without overlapping data: myth or reality?. In Proceedings of the 8th ACM Conference on Recommender systems (pp. 297-300). ACM.</p>

<p>❏ Li, C. Y., &amp; Lin, S. D. (2014, August). Matching users and items across domains to improve the recommendation quality. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 801-810). ACM.</p>

<p>❏ Li, B., Yang, Q., &amp; Xue, X. (2009, July). Can Movies and Books Collaborate? Cross-Domain Collaborative Filtering for Sparsity Reduction. In IJCAI (Vol. 9, pp. 2052-2057).</p>

<p>✔ Li, B. (2011, November). Cross-domain collaborative filtering: A brief survey. In Tools with Artificial Intelligence (ICTAI), 2011 23rd IEEE International Conference on (pp. 1085-1086). IEEE.</p>

<p>✔Gao, S., Luo, H., Chen, D., Li, S., Gallinari, P., &amp; Guo, J. (2013). Cross-Domain Recommendation via Cluster-Level Latent Factor Model. In Machine Learning and Knowledge Discovery in Databases (pp. 161-176). Springer Berlin Heidelberg.</p>

<p>❏ Li, B., Yang, Q., &amp; Xue, X. (2009, June). Transfer learning for collaborative filtering via a rating-matrix generative model. In Proceedings of the 26th Annual International Conference on Machine Learning (pp. 617-624). ACM.</p>

<p>❏ Moreno, O., Shapira, B., Rokach, L., &amp; Shani, G. (2012, October). Talmud: transfer learning for multiple domains. In Proceedings of the 21st ACM international conference on Information and knowledge management (pp. 425-434). ACM.</p>

<p>❏ Li, B., Yang, Q., &amp; Xue, X. (2009, July). Can Movies and Books Collaborate? Cross-Domain Collaborative Filtering for Sparsity Reduction. In IJCAI (Vol. 9, pp. 2052-2057).</p>

<p>✔ Loni, B., Shi, Y., Larson, M., &amp; Hanjalic, A. (2014). Cross-Domain Collaborative Filtering with Factorization Machines. In Advances in Information Retrieval (pp. 656-661). Springer International Publishing.</p>

<p>❏ Fernández-Tobías, I., Cantador, I., Kaminskas, M., &amp; Ricci, F. (2012). Cross-domain recommender systems: A survey of the state of the art. In Proceedings of the 2nd Spanish Conference on Information Retrieval. CERI.</p>

<p>❏ Pan, W., &amp; Ming, Z. (2014). Interaction-Rich Transfer Learning for Collaborative Filtering with Heterogeneous User Feedbacks. IEEE Intelligent Systems, 1.</p>

<p>❏Shi, Y., Larson, M., &amp; Hanjalic, A. (2014). Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges. ACM Computing Surveys (CSUR), 47(1), 3.</p>

<p>❏ Low, Y., Agarwal, D., &amp; Smola, A. J. (2011, August). Multiple domain user personalization. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 123-131). ACM.</p>

<p>❏ Berkovsky, S., Goldwasser, D., Kuflik, T., &amp; Ricci, F. (2006, May). Identifying Inter-Domain Similarities Through Content-Based Analysis of Hierarchical Web-Directories. In ECAI (pp. 789-790).</p>

<p>❏ Dr. Weike Pan’ paper s</p>

<p>❏ Hu, L., Cao, J., Xu, G., Wang, J., Gu, Z., &amp; Cao, L. (2013, August). Cross-domain collaborative filtering via bilinear multilevel analysis. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence (pp. 2626-2632). AAAI Press.</p>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2014-10-28T00:00:00-04:00"><a href="http://boliu68.github.io/how%20to%20compile%20OpenCV%20without%20root/">October 28, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://boliu68.github.io/about/" title="About Bo Liu">Bo Liu</a></span></span>
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
        Reading time ~1 minute
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://boliu68.github.io/how%20to%20compile%20OpenCV%20without%20root/" rel="bookmark" title="How to compile opencv on linux without root" itemprop="url">How to compile opencv on linux without root</a></h1>
    
  </header>
  <div class="entry-content">
    <h2 id="update-cmake">update Cmake</h2>

<ol>
  <li>only cmake version &gt; 2.88 can be used to compile opencv.</li>
  <li>Download latest version cmake source code and cd in the folder.</li>
  <li>./bootstrap</li>
  <li>make (No need to make install, just executate new cmake from built dir)</li>
</ol>

<hr />
<p>##Compiling OpenCV</p>

<ol>
  <li>Download the latest version opencv from website.</li>
  <li>Mkdir build inside the unzip opencv folder.</li>
  <li>cmake(execute the newly installed cmake using direct path)</li>
</ol>

<p>The command(do not ignore <strong>..</strong>):</p>

<pre><code>cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=my_location -D BUILD_NEW_PYTHON_SUPPORT=ON ..
</code></pre>

<ol>
  <li>
    <p>make</p>
  </li>
  <li>make install</li>
  <li>Guarantee that python2.7 folder is generated in lib.</li>
  <li>config the environment.</li>
</ol>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->

<article class="hentry">
  <header>
    
    <div class="entry-meta">
      <span class="entry-date date published updated"><time datetime="2014-10-14T00:00:00-04:00"><a href="http://boliu68.github.io/What%20means%20by%20Sparsity%3F/">October 14, 2014</a></time></span><span class="author vcard"><span class="fn"><a href="http://boliu68.github.io/about/" title="About Bo Liu">Bo Liu</a></span></span>
      
      <span class="entry-reading-time">
        <i class="fa fa-clock-o"></i>
        
        Reading time ~4 minutes
      </span><!-- /.entry-reading-time -->
      
    </div><!-- /.entry-meta -->
    
      <h1 class="entry-title"><a href="http://boliu68.github.io/What%20means%20by%20Sparsity%3F/" rel="bookmark" title="What mean by sparsity?" itemprop="url">What mean by sparsity?</a></h1>
    
  </header>
  <div class="entry-content">
    <p>In machine learning area, we hear a lot about sparsity such as compressed sensing, sparse coding, data sparsity and so on. Amongst, I am particular interested in data sparsity. In recommender system, the large proportion of missing ratings is called sparsity. On the other hand, the bag of word(BoG) represented document dataset is also named sparse data. What exactly means by sparsity is the fundamental problem for us to understand as well as utilize data sparsity. My original confusion is posted on <a href="http://stats.stackexchange.com/questions/113318/the-name-data-sparsity-in-different-applications" title="Cross Validated">Cross Validated</a>.</p>

<p>In this blog, I hope to list the taxonomy and several definition from very intuitive perspective. Moreover, I would like to emphasize that this blog only talks about sparse data instead of sparse learning. The object method aims at learning a sparse parameter.</p>

<p>First of all, I want to clarify two types of data sparsity<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>.</p>

<ol>
  <li>Paucity of dataset particular training data.</li>
  <li>High dimension feature space and only few dimensions are informative for each instance.</li>
</ol>

<p><strong>Paucity of training data</strong>: Take the <a href="http://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize</a> dataset as example again. The large proportion of missing ratings makes recovering the user-movie matrix more challenging. The sparsity ratio, <script type="math/tex">\frac{\#observerd}{\# user \times \# movies}</script>, is always utilized how sparse the dataset is. Lee, J<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> provides an empirical analysis the relation between performance and density/sparisty. The insufficient training data situaiton is quite common.</p>

<p><strong>High dimension feature space</strong>: Take the linear classifier for binary classification as example. As we known, the VC dimension of linear classifier is <script type="math/tex">O(d)</script> where <script type="math/tex">d</script> is feature dimension. As the dimension increase, the number of instance is required to grow as well.</p>

<p>The high dimensionality also results in some other problems known as <strong>curse of dimensionality</strong>. The sparsity is only one of consequences of curse of dimension. The distance becomes meaningless as dimension increase. However, the discusion of curse of dimension is beyond this post.</p>

<p>In text mining, bag of word is always utilized to represent each domument like the following.</p>

<p>\[[0,1,0,0,0\dots,0,0,0,\dots,1,0,\dots]\]</p>

<p>We usually call such instance sparse caused only few entries are non-zero. However, such understanding is quite on the surface. As I actually cannot explain the difference between <script type="math/tex">[0,1,0,0,1]</script> and <script type="math/tex">[1,2,1,1,2]$.</script> However, the latter one will not be called sparse. Duchi, J.<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup> provides a formal condition for sparse as 
\[\text{for convex loss function } f,supp(f) \subset supp(\nabla f)\]</p>

<p>From intuitive perspective, such condition means that the <script type="math/tex">i\_{th}</script> of instance dimension <script type="math/tex">x</script> is zero <script type="math/tex">x_i=0</script>, then the gradient with respect to <script type="math/tex">x_i</script> is deemed to be zero. Consider using stochastic gradient descent to learn the model. The instance with sparse features $x_i=0$ will not update <script type="math/tex">i\_{th}</script> dimension at all. Duchi,</p>

<p>J.<sup id="fnref:3:1"><a href="#fn:3" class="footnote">3</a></sup> also provides an analysis of stochastic optimization on such sparse data. When learning on such sparse dataset, we would like to learn an dense model rather than sparse model(assumption). Such assmption leads much harder to learn model on sparse data.</p>

<p><strong>Other definition</strong></p>

<p>At the very begining of my survey. I always try to make an uniform definition for the above two type of sparsity. Actually, there indeed exist some common parts.</p>

<ol>
  <li>Both the paucity of training data and high dimension will cause the average distance between each data point large.</li>
  <li>The <a href="http://math.stackexchange.com/questions/283006/what-is-a-sampling-density-why-is-the-sampling-density-proportional-to-n-fra">sample density</a> is also tried to measure the sparsity.</li>
</ol>

<p>In the following, I try to quote several “definition” in other references.</p>

<ol>
  <li>
    <blockquote>
      <p>Data sparsity refers to the difficulty in finding sufficient reliable similar users since in general the active users only rated a small portion of items<sup id="fnref:5"><a href="#fn:5" class="footnote">4</a></sup></p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>data sparsity problem occus in the setting of supervised statistical learning method, when some data from the test side is not present in the training dataset. <sup id="fnref:6"><a href="#fn:6" class="footnote">5</a></sup></p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>when the training data is relatively sparse in this domain, either because few observations are available for learning or because the underlying structure is complex, the bias inherent in popular pruning methods is inappropriate and they have a negative effect on predictive accuracy<sup id="fnref:4"><a href="#fn:4" class="footnote">6</a></sup>.</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>One way to think of sparsity is how space is empty (60%), whereas 40% of space is dense, or filled. So good data looks like swiss cheese.  Big bubbles of nothing<sup id="fnref:7"><a href="#fn:7" class="footnote">7</a></sup>!</p>
    </blockquote>
  </li>
</ol>

<p>There exist quite a lot definition or understanding of data sparsity. As far as I have survied, I do not find a very persuasive and comprehensive work to clarify all of these definitions.</p>

<h2 id="references">References</h2>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>David Donoho, Sparsity in Modern High-Dimensional Statistics, SAMSI Astrostatistics. 20,9,2012 <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Lee, J., Sun, M., &amp; Lebanon, G. (2012). A comparative study of collaborative filtering algorithms. arXiv preprint arXiv:1205.3193. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Duchi, J., Jordan, M., &amp; McMahan, B. (2013). Estimation, optimization, and parallelism when data is sparse. In Advances in Neural Information Processing Systems (pp. 2832-2840). <a href="#fnref:3" class="reversefootnote">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:5">
      <p>Guo, G. (2013, August). Improving the performance of recommender systems by alleviating the data sparsity and cold start problems. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence (pp. 3217-3218). AAAI Press. <a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Ricci, F., Rokach, L., &amp; Shapira, B. (2011). Introduction to recommender systems handbook (pp. 1-35). Springer US. <a href="#fnref:6" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>when the training data is relatively sparse in this domain, either because few observations are available for learning or because the underlying structure is complex, the bias inherent in popular pruning methods is inappropriate and they have a negative effect on predictive accuracy. <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>http://www.quora.com/What-is-a-clear-explanation-of-data-sparsity <a href="#fnref:7" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div><!-- /.entry-content -->
</article><!-- /.hentry -->


<div class="pagination">
  
    
      <a href="http://boliu68.github.io" class="btn">Previous</a>
    
  
  <ul class="inline-list">
    <li>
      
        <a href="http://boliu68.github.io">1</a>
      
    </li>
    
      <li>
        
          <span class="current-page">2</span>
        
      </li>
    
  </ul>
  
    Next
  
</div><!-- /.pagination -->
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2015 Bo Liu. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://mademistakes.com/work/hpstr-jekyll-theme/" rel="nofollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://boliu68.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://boliu68.github.io/assets/js/scripts.min.js"></script>



          

</body>
</html>