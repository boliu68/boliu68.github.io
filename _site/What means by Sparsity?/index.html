<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>What mean by sparsity? &#8211; Long Long Later</title>
<meta name="description" content="Blog of Bo Liu">
<meta name="keywords" content="survey, sparsity">



<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="What mean by sparsity?">
<meta property="og:description" content="Blog of Bo Liu">
<meta property="og:url" content="http://boliu68.github.io/What%20means%20by%20Sparsity%3F/">
<meta property="og:site_name" content="Long Long Later">





<link rel="canonical" href="http://boliu68.github.io/What%20means%20by%20Sparsity%3F/">
<link href="http://boliu68.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Long Long Later Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- For all browsers -->
<link rel="stylesheet" href="http://boliu68.github.io/assets/css/main.css">
<!-- Webfonts -->
<link href="//fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic" rel="stylesheet" type="text/css">

<meta http-equiv="cleartype" content="on">

<!-- Load Modernizr -->
<script src="http://boliu68.github.io/assets/js/vendor/modernizr-2.6.2.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://boliu68.github.io/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://boliu68.github.io/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://boliu68.github.io/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://boliu68.github.io/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://boliu68.github.io/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://boliu68.github.io/images/apple-touch-icon-144x144-precomposed.png">



</head>

<body id="post" >

<!--[if lt IE 9]><div class="upgrade"><strong><a href="http://whatbrowser.org/">Your browser is quite old!</strong> Why not upgrade to a different browser to better enjoy this site?</a></div><![endif]-->
<nav id="dl-menu" class="dl-menuwrapper" role="navigation">
	<button class="dl-trigger">Open Menu</button>
	<ul class="dl-menu">
		<li><a href="http://boliu68.github.io/">Home</a></li>
		<li>
			<a href="#">About</a>
			<ul class="dl-submenu">
				<li>
					<img src="http://boliu68.github.io/images/head.jpg" alt="Bo Liu photo" class="author-photo">
					<h4>Bo Liu</h4>
					<p>Machine Learning, PhD student</p>
				</li>
				<li><a href="http://boliu68.github.io/about/"><span class="btn btn-inverse">Learn More</span></a></li>
				<li>
					<a href="mailto:6liubo8@gmail.com"><i class="fa fa-fw fa-envelope"></i> Email</a>
				</li>
				
				
				
				<li>
					<a href="http://linkedin.com/in/https://hk.linkedin.com/pub/bo-brody-liu/55/52b/10b"><i class="fa fa-fw fa-linkedin"></i> LinkedIn</a>
				</li>
				
				
				
				
				
			</ul><!-- /.dl-submenu -->
		</li>
		<li>
			<a href="#">Posts</a>
			<ul class="dl-submenu">
				<li><a href="http://boliu68.github.io/posts/">All Posts</a></li>
				<li><a href="http://boliu68.github.io/tags/">All Tags</a></li>
			</ul>
		</li>
		
	</ul><!-- /.dl-menu -->
</nav><!-- /.dl-menuwrapper -->




<div id="main" role="main">
  <article class="hentry">
    <header class="header-title">
      <div class="header-title-wrap">
        
          <h1 class="entry-title"><a href="http://boliu68.github.io/What%20means%20by%20Sparsity%3F/" rel="bookmark" title="What mean by sparsity?">What mean by sparsity?</a></h1>
        
        <h2><span class="entry-date date published"><time datetime="2014-10-14T00:00:00-04:00">October 14, 2014</time></span></h2>
        
        <p class="entry-reading-time">
          <i class="fa fa-clock-o"></i>
          
          Reading time ~4 minutes
        </p><!-- /.entry-reading-time -->
        
      </div><!-- /.header-title-wrap -->
    </header>
    <div class="entry-content">
      <p>What mean by sparsity?</p>

<p>In machine learning area, we hear a lot about sparsity such as compressed sensing, sparse coding, data sparsity and so on. Amongst, I am particular interested in data sparsity. In recommender system, the large proportion of missing ratings is called sparsity. On the other hand, the bag of word(BoG) represented document dataset is also named sparse data. What exactly means by sparsity is the fundamental problem for us to understand as well as utilize data sparsity. My original confusion is posted on <a href="http://stats.stackexchange.com/questions/113318/the-name-data-sparsity-in-different-applications" title="Cross Validated">Cross Validated</a>.</p>

<p>In this blog, I hope to list the taxonomy and several definition from very intuitive perspective. Moreover, I would like to emphasize that this blog only talks about sparse data instead of sparse learning. The object method aims at learning a sparse parameter.</p>

<p>First of all, I want to clarify two types of data sparsity<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>.</p>

<ol>
  <li>Paucity of dataset particular training data.</li>
  <li>High dimension feature space and only few dimensions are informative for each instance.</li>
</ol>

<p><strong>Paucity of training data</strong>: Take the <a href="http://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize</a> dataset as example again. The large proportion of missing ratings makes recovering the user-movie matrix more challenging. The sparsity ratio, <script type="math/tex">\frac{\#observerd}{\# user \times \# movies}</script>, is always utilized how sparse the dataset is. Lee, J<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> provides an empirical analysis the relation between performance and density/sparisty. The insufficient training data situaiton is quite common.</p>

<p><strong>High dimension feature space</strong>: Take the linear classifier for binary classification as example. As we known, the VC dimension of linear classifier is <script type="math/tex">O(d)</script> where <script type="math/tex">d</script> is feature dimension. As the dimension increase, the number of instance is required to grow as well.</p>

<p>The high dimensionality also results in some other problems known as <strong>curse of dimensionality</strong>. The sparsity is only one of consequences of curse of dimension. The distance becomes meaningless as dimension increase. However, the discusion of curse of dimension is beyond this post.</p>

<p>In text mining, bag of word is always utilized to represent each domument like the following.</p>

<p>\[[0,1,0,0,0\dots,0,0,0,\dots,1,0,\dots]\]</p>

<p>We usually call such instance sparse caused only few entries are non-zero. However, such understanding is quite on the surface. As I actually cannot explain the difference between <script type="math/tex">[0,1,0,0,1]</script> and <script type="math/tex">[1,2,1,1,2]$.</script> However, the latter one will not be called sparse. Duchi, J.<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup> provides a formal condition for sparse as
\[\text{for convex loss function } f,supp(f) \subset supp(\nabla f)\]</p>

<p>From intuitive perspective, such condition means that the <script type="math/tex">i\_{th}</script> of instance dimension <script type="math/tex">x</script> is zero <script type="math/tex">x_i=0</script>, then the gradient with respect to <script type="math/tex">x_i</script> is deemed to be zero. Consider using stochastic gradient descent to learn the model. The instance with sparse features $x_i=0$ will not update <script type="math/tex">i\_{th}</script> dimension at all. Duchi,</p>

<p>J.<sup id="fnref:3:1"><a href="#fn:3" class="footnote">3</a></sup> also provides an analysis of stochastic optimization on such sparse data. When learning on such sparse dataset, we would like to learn an dense model rather than sparse model(assumption). Such assmption leads much harder to learn model on sparse data.</p>

<p><strong>Other definition</strong></p>

<p>At the very begining of my survey. I always try to make an uniform definition for the above two type of sparsity. Actually, there indeed exist some common parts.</p>

<ol>
  <li>Both the paucity of training data and high dimension will cause the average distance between each data point large.</li>
  <li>The <a href="http://math.stackexchange.com/questions/283006/what-is-a-sampling-density-why-is-the-sampling-density-proportional-to-n-fra">sample density</a> is also tried to measure the sparsity.</li>
</ol>

<p>In the following, I try to quote several “definition” in other references.</p>

<ol>
  <li>
    <blockquote>
      <p>Data sparsity refers to the difficulty in finding sufficient reliable similar users since in general the active users only rated a small portion of items<sup id="fnref:5"><a href="#fn:5" class="footnote">4</a></sup></p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>data sparsity problem occus in the setting of supervised statistical learning method, when some data from the test side is not present in the training dataset. <sup id="fnref:6"><a href="#fn:6" class="footnote">5</a></sup></p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>when the training data is relatively sparse in this domain, either because few observations are available for learning or because the underlying structure is complex, the bias inherent in popular pruning methods is inappropriate and they have a negative effect on predictive accuracy<sup id="fnref:4"><a href="#fn:4" class="footnote">6</a></sup>.</p>
    </blockquote>
  </li>
  <li>
    <blockquote>
      <p>One way to think of sparsity is how space is empty (60%), whereas 40% of space is dense, or filled. So good data looks like swiss cheese.  Big bubbles of nothing<sup id="fnref:7"><a href="#fn:7" class="footnote">7</a></sup>!</p>
    </blockquote>
  </li>
</ol>

<p>There exist quite a lot definition or understanding of data sparsity. As far as I have survied, I do not find a very persuasive and comprehensive work to clarify all of these definitions.</p>

<h2 id="references">References</h2>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>David Donoho, Sparsity in Modern High-Dimensional Statistics, SAMSI Astrostatistics. 20,9,2012 <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Lee, J., Sun, M., &amp; Lebanon, G. (2012). A comparative study of collaborative filtering algorithms. arXiv preprint arXiv:1205.3193. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Duchi, J., Jordan, M., &amp; McMahan, B. (2013). Estimation, optimization, and parallelism when data is sparse. In Advances in Neural Information Processing Systems (pp. 2832-2840). <a href="#fnref:3" class="reversefootnote">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:5">
      <p>Guo, G. (2013, August). Improving the performance of recommender systems by alleviating the data sparsity and cold start problems. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence (pp. 3217-3218). AAAI Press. <a href="#fnref:5" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:6">
      <p>Ricci, F., Rokach, L., &amp; Shapira, B. (2011). Introduction to recommender systems handbook (pp. 1-35). Springer US. <a href="#fnref:6" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>when the training data is relatively sparse in this domain, either because few observations are available for learning or because the underlying structure is complex, the bias inherent in popular pruning methods is inappropriate and they have a negative effect on predictive accuracy. <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:7">
      <p>http://www.quora.com/What-is-a-clear-explanation-of-data-sparsity <a href="#fnref:7" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

      <footer class="entry-meta">
        <span class="entry-tags"><a href="http://boliu68.github.io/tags/#survey" title="Pages tagged survey" class="tag"><span class="term">survey</span></a><a href="http://boliu68.github.io/tags/#sparsity" title="Pages tagged sparsity" class="tag"><span class="term">sparsity</span></a></span>
        <span>Updated on <span class="entry-date date updated"><time datetime="2014-10-14">October 14, 2014</time></span></span>
        <span class="author vcard"><span class="fn">Bo Liu</span></span>
        <div class="social-share">
  <ul class="socialcount socialcount-small inline-list">
    <li class="facebook"><a href="https://www.facebook.com/sharer/sharer.php?u=http://boliu68.github.io/What%20means%20by%20Sparsity%3F/" title="Share on Facebook"><span class="count"><i class="fa fa-facebook-square"></i> Like</span></a></li>
    <li class="twitter"><a href="https://twitter.com/intent/tweet?text=http://boliu68.github.io/What%20means%20by%20Sparsity%3F/" title="Share on Twitter"><span class="count"><i class="fa fa-twitter-square"></i> Tweet</span></a></li>
    <li class="googleplus"><a href="https://plus.google.com/share?url=http://boliu68.github.io/What%20means%20by%20Sparsity%3F/" title="Share on Google Plus"><span class="count"><i class="fa fa-google-plus-square"></i> +1</span></a></li>
  </ul>
</div><!-- /.social-share -->
      </footer>
    </div><!-- /.entry-content -->
    <section id="disqus_thread"></section><!-- /#disqus_thread -->
    <div class="read-more">
  
    <div class="read-more-header">
      <a href="http://boliu68.github.io" class="read-more-btn">Read More</a>
    </div><!-- /.read-more-header -->
    <div class="read-more-content">
      <h3><a href="http://boliu68.github.io/Savage's%20approach%20to%20research/" title="Savage's approach to research">Savage's approach to research</a></h3>
      <p>Savage's approach to research, via Mosteller（copy from Jon McAuliffe's and Jun Liu’s webpage):- As soon as a problem is stated, start rig...&hellip; <a href="http://boliu68.github.io/Savage's%20approach%20to%20research/">Continue reading</a></p>
    </div><!-- /.read-more-content -->
  
  <div class="read-more-list">
    
      <div class="list-item">
        <h4><a href="http://boliu68.github.io/Notes%20on%20Sure%20screening%20interaction%20selection/" title="Notes on Sure Screening interaction selection">Notes on Sure Screening interaction selection</a></h4>
        <span>Published on September 08, 2015</span>
      </div><!-- /.list-item -->
    
      <div class="list-item">
        <h4><a href="http://boliu68.github.io/ReadList%20of%20Distributed%20Optimization%20Algorithms/" title="Readlist of Distributed Optimization Algorithm">Readlist of Distributed Optimization Algorithm</a></h4>
        <span>Published on May 10, 2015</span>
      </div><!-- /.list-item -->
    
  </div><!-- /.read-more-list -->
</div><!-- /.read-more -->
  </article>
</div><!-- /#main -->

<div class="footer-wrapper">
  <footer role="contentinfo">
    <span>&copy; 2015 Bo Liu. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> using the <a href="https://mademistakes.com/work/hpstr-jekyll-theme/" rel="nofollow">HPSTR Theme</a>.</span>
  </footer>
</div><!-- /.footer-wrapper -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="http://boliu68.github.io/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://boliu68.github.io/assets/js/scripts.min.js"></script>




    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'hpstrtheme'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script'); s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
	        

</body>
</html>
