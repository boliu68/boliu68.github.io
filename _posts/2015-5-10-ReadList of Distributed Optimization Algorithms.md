---
layout: post
categories: [survey, readlist]

---

#Readlist of Distributed Optimization Algorithm

[1] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trend in Machine Learning, 3(1):1–122, 2011.
[2] Joseph K Bradley, Aapo Kyrola, Danny Bickson, and Carlos Guestrin. Par- allel coordinate descent for l1-regularized loss minimization. arXiv preprint arXiv:1105.5379, 2011.
[3] Weizhu Chen, Zhenghao Wang, and Jingren Zhou. Large-scale l-bfgs using mapreduce. In Advances in Neural Information Processing Systems, pages 1332–1340, 2014.
[4] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al. Large scale distributed deep networks. In Advances in Neural Information Processing Systems, pages 1223–1231, 2012.
[5] Yi Wang, Hongjie Bai, Matt Stanton, Wen-Yen Chen, and Edward Y Chang. Plda: Parallel latent dirichlet allocation for large-scale applications. In Al- gorithmic aspects in information and management, pages 301–314. Springer, 2009.
[6] Yong Zhuang, Wei-Sheng Chin, Yu-Chin Juan, and Chih-Jen Lin. Dis- tributed newton method for regularized logistic regression. Department of Computer Science and Information Engineering, National Taiwan University, Tech. Rep, 2014.
[7] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Par- allelized stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 2595–2603, 2010.